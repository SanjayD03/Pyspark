ğ‘·ğ’šğ‘ºğ’‘ğ’‚ğ’“ğ’Œ ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ğ’”:
------------------------------------------------------------------------------------------
 1. ğ…ğ¢ğ¥ğ­ğğ« ğ­ğ¡ğ ğƒğšğ­ğšğ…ğ«ğšğ¦ğ ğ°ğ¡ğğ«ğ ğšğ ğ ğ¢ğ¬ ğ ğ«ğğšğ­ğğ« ğ­ğ¡ğšğ§ 28. 
------------------------------------------------------------------------------------------
data = [(1, "John", 30), (2, "Jane", 25), (3, "Mike", 35)]
columns = ["id", "name", "age"]
df = spark.createDataFrame(data, columns)

ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:
df.filter(col("age")>28).show()

------------------------------------------------------------------------------------------
2. ğˆğğğ§ğ­ğ¢ğŸğ² ğğ®ğ©ğ¥ğ¢ğœğšğ­ğ ğ«ğğœğ¨ğ«ğğ¬ ğ›ğšğ¬ğğ ğ¨ğ§ ğ­ğ¡ğ ğ§ğšğ¦ğ ğœğ¨ğ¥ğ®ğ¦ğ§.
------------------------------------------------------------------------------------------
Sample Data: 
data = [(1, "Alice"), (2, "Bob"), (3, "Alice"), (4, "David"), (2, "Bob")]
columns = ["id", "name"]
df = spark.createDataFrame(data, columns)

ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:
window_spec = Window.partitionBy("name").orderBy("id")
df.withColumn("Row Number",row_number()\
.over(window_spec)).filter(col("Row Number")>1)\
.select("id","name").show()

------------------------------------------------------------------------------------------
3. ğ‘ğğ¦ğ¨ğ¯ğ ğğ®ğ©ğ¥ğ¢ğœğšğ­ğğ¬ ğŸğ«ğ¨ğ¦ ğ­ğ¡ğ ğšğ›ğ¨ğ¯ğ ğƒğšğ­ğšğ…ğ«ğšğ¦ğ ğ°ğ¡ğ¢ğ¥ğ ğ¤ğğğ©ğ¢ğ§ğ  ğ­ğ¡ğ ğŸğ¢ğ«ğ¬ğ­ ğ¨ğœğœğ®ğ«ğ«ğğ§ğœğ.
------------------------------------------------------------------------------------------
ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:

window_spec = Window.partitionBy("name").orderBy("id")
df.withColumn("Row Number",row_number().over(window_spec)).filter(col("Row Number")==1).select("id","name").show()

- Approach 2:
df_no_duplicates = df.dropDuplicates(["name"])
df_no_duplicates.show()

------------------------------------------------------------------------------------------
4. ğ–ğ«ğ¢ğ­ğ ğš ğğ²ğ’ğ©ğšğ«ğ¤ ğªğ®ğğ«ğ² ğ­ğ¨ ğ ğğ­ ğ­ğ¡ğ ğ¬ğğœğ¨ğ§ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğ¬ğšğ¥ğšğ«ğ².
------------------------------------------------------------------------------------------
data = [("John", 5000), ("Jane", 7000), ("Mike", 6000), ("Alice", 8000)]
columns = ["name", "salary"]
df = spark.createDataFrame(data, columns)

ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:
from pyspark.sql.functions import *
from pyspark.sql.window import Window
window_spec = Window.orderBy(col("salary").desc())
df.withColumn("Dense Rank",dense_rank().over(window_spec)).filter(col("Dense Rank")==2).drop(col("Dense Rank")).show()
------------------------------------------------------------------------------------------
5.ğ–ğ«ğ¢ğ­ğ ğš ğğ²ğ’ğ©ğšğ«ğ¤ ğªğ®ğğ«ğ² ğ­ğ¨ ğ ğğ­ ğ­ğ¡ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­-ğ©ğšğ¢ğ ğğ¦ğ©ğ¥ğ¨ğ²ğğ ğ¢ğ§ ğğšğœğ¡ ğğğ©ğšğ«ğ­ğ¦ğğ§ğ­.
------------------------------------------------------------------------------------------
data = [("John", "HR", 5000), ("Jane", "IT", 7000), ("Mike", "HR", 6000), ("Alice", "IT", 8000)]
columns = ["name", "dept", "salary"]
df = spark.createDataFrame(data, columns)

ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:
from pyspark.sql.functions import *
from pyspark.sql.window import Window
df.groupBy("dept").agg(max("salary").alias("Highest_paid")).show()

------------------------------------------------------------------------------------------
6. ğ„ğ±ğ©ğ¥ğ¨ğğ ğ­ğ¡ğ ğ¬ğ¤ğ¢ğ¥ğ¥ğ¬ ğœğ¨ğ¥ğ®ğ¦ğ§ ğ­ğ¨ ğ¡ğšğ¯ğ ğ¨ğ§ğ ğ¬ğ¤ğ¢ğ¥ğ¥ ğ©ğğ« ğ«ğ¨ğ°.
------------------------------------------------------------------------------------------
data = [("John", ["Python", "Java"]), ("Jane", ["Scala", "Spark"])]
columns = ["name", "skills"]
df = spark.createDataFrame(data, columns)

ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:
df_exploded = df.withColumn("skill", explode("skills")).drop("skills")
df_exploded.show()
------------------------------------------------------------------------------------------
7. ğ‚ğ¨ğ§ğ¯ğğ«ğ­ ğğğ©ğšğ«ğ­ğ¦ğğ§ğ­ ğ§ğšğ¦ğğ¬ ğ¢ğ§ğ­ğ¨ ğœğ¨ğ¥ğ®ğ¦ğ§ğ¬ ğ°ğ¢ğ­ğ¡ ğ­ğ¨ğ­ğšğ¥ ğ¬ğšğ¥ğšğ«ğ² ğšğ¬ ğ¯ğšğ¥ğ®ğğ¬.
------------------------------------------------------------------------------------------
data = [("John", "HR", 5000), ("Jane", "IT", 7000), ("Mike", "HR", 6000)]
columns = ["name", "dept", "salary"]
df = spark.createDataFrame(data, columns)

ğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§:
pivot_df = df.groupBy().pivot("dept").agg(sum("salary"))
pivot_df.show()

