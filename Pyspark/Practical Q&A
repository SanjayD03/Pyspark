PySpark Questions:
======================
------------------------------------------------------------------------------------------
1. Filter the DataFrame where age is greater than 28. 
------------------------------------------------------------------------------------------
data = [(1, "John", 30), (2, "Jane", 25), (3, "Mike", 35)]
columns = ["id", "name", "age"]
df = spark.createDataFrame(data, columns)

Solution:
df.filter(col("age")>28).show()

------------------------------------------------------------------------------------------
2. Identify duplicate records based on the name column.
------------------------------------------------------------------------------------------
Sample Data: 
data = [(1, "Alice"), (2, "Bob"), (3, "Alice"), (4, "David"), (2, "Bob")]
columns = ["id", "name"]
df = spark.createDataFrame(data, columns)

Solution:
window_spec = Window.partitionBy("name").orderBy("id")
df.withColumn("Row Number",row_number()\
.over(window_spec)).filter(col("Row Number")>1)\
.select("id","name").show()

------------------------------------------------------------------------------------------
3. Remove duplicates from the above DataFrame while keeping the first occurrence.
------------------------------------------------------------------------------------------
Solution:
- Approach:1
window_spec = Window.partitionBy("name").orderBy("id")
df.withColumn("Row Number",row_number().over(window_spec)).filter(col("Row Number")==1).select("id","name").show()

- Approach 2:
df_no_duplicates = df.dropDuplicates(["name"])
df_no_duplicates.show()

------------------------------------------------------------------------------------------
4. Write a PySpark query to get the second highest salary.
------------------------------------------------------------------------------------------
data = [("John", 5000), ("Jane", 7000), ("Mike", 6000), ("Alice", 8000)]
columns = ["name", "salary"]
df = spark.createDataFrame(data, columns)

Solution:
from pyspark.sql.functions import *
from pyspark.sql.window import Window
window_spec = Window.orderBy(col("salary").desc())
df.withColumn("Dense Rank",dense_rank().over(window_spec)).filter(col("Dense Rank")==2).drop(col("Dense Rank")).show()
------------------------------------------------------------------------------------------
5.Write a PySpark query to get the highest-paid employee in each department.
------------------------------------------------------------------------------------------
data = [("John", "HR", 5000), ("Jane", "IT", 7000), ("Mike", "HR", 6000), ("Alice", "IT", 8000)]
columns = ["name", "dept", "salary"]
df = spark.createDataFrame(data, columns)

Solution:
from pyspark.sql.functions import *
from pyspark.sql.window import Window
df.groupBy("dept").agg(max("salary").alias("Highest_paid")).show()

------------------------------------------------------------------------------------------
6. Explode the skills column to have one skill per row.
------------------------------------------------------------------------------------------
data = [("John", ["Python", "Java"]), ("Jane", ["Scala", "Spark"])]
columns = ["name", "skills"]
df = spark.createDataFrame(data, columns)

Solution:

------------------------------------------------------------------------------------------
7. Convert department names into columns with total salary as values.
------------------------------------------------------------------------------------------
data = [("John", "HR", 5000), ("Jane", "IT", 7000), ("Mike", "HR", 6000)]
columns = ["name", "dept", "salary"]
df = spark.createDataFrame(data, columns)

Solution:


